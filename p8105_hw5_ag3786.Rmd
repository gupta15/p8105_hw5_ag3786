---
title: "Homework 5: Iteration"
author: "Aakriti Gupta"
date: "November 9, 2018"
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

library(tidyverse)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```


```{r, include=FALSE}
library(stringr)
library(purrr)
library(viridis)
```

# Problem 1
These data are from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

## Create a tidy dataframe 
Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time

* Start with a dataframe containing all file names; the list.files function will help
* Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
* Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary


```{r, warning=FALSE, message=FALSE}
## Creating a list of participant data file names
file_name_list = list.files("./data")

## Creating a function to read the csv files and add the respective file names
read_csv_iteration = function(data_files) {
  subject_data = 
    read_csv(file = str_c("./data/", data_files)) %>% 
    mutate(file = data_files)
}

## Reading in all the files in a combined dataset using a function and tidying the data
data_combined = 
  map_df(file_name_list, read_csv_iteration) %>% 
  gather(key = week, value = observation, week_1:week_8) %>% 
  separate(file, into = c("group", "subject"), sep = "_") %>% 
  mutate(week = str_replace(week, "week_", ""),
         subject = str_replace(subject,".csv",""),
         group = recode(group, "con" = "control", 
                        "exp" = "experimental"))

```

## Make a spaghetti plot 
* Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups

```{r}
data_combined %>% 
  mutate(group = recode(group, "control" = "Control", "experimental" = "Experimental")) %>% 
  ggplot(aes(x = week, y = observation, group = subject, color = subject)) + 
    geom_path() + 
    facet_grid(~group) + 
    labs(
      title = "Observations of Subjects over Time",
      x = "Week",
      y = "Observation"
    ) + 
    theme(legend.position = "bottom") +
    viridis::scale_color_viridis(
      name = "Subject ID", 
      discrete = TRUE
    )
```

These plots demonstrate that the observations of subjects in the control group remained fairly constant through the study period. On the other hand, the observations of subjects in the experimental group increased in value from week 1 through week 8.

# Problem 2
The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository.

## Reading in the data from the GitHub repository

```{r}
wapo_homicide_url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

wapo_homicide = 
  read.csv(url(wapo_homicide_url)) %>% 
  janitor::clean_names() %>% 
  as_tibble()
```

## Describe the raw data
This dataset has `r nrow(wapo_homicide)` rows and `r ncol(wapo_homicide)` columns. Each row contains information about a single homicide that has occurred, including:

* Location (city, state, latitude/longitude) 
* Information about the victim (first/last name, age, sex, disposition)
* Information about the homicide (disposition, reported date, id). 

These data have been reported from `r count(wapo_homicide, city) %>% nrow()` cities in `r count(wapo_homicide, state) %>% nrow()` states. `r round(wapo_homicide %>% filter(disposition == "Closed by arrest") %>% nrow() / nrow(wapo_homicide) * 100)`% of the homicides were solved by arrest and the remaining are considered unsolved (solved with no arrest or still open/no arrest). `r round(wapo_homicide %>% filter(victim_sex == "Female") %>% nrow() / nrow(wapo_homicide) * 100)`% of the homicides had female victims. 

## Number of total and unsolved homicides by city
Create a `city_state` variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
  wapo_homicide_summary = wapo_homicide %>% 
  mutate(city_state = str_c(city, state, sep = ", ")) %>% 
  group_by(city_state) %>% 
  mutate(total_homicides_city = n()) %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>% 
  summarise(unsolved_homicides = n(),
            total_homicides = max(total_homicides_city)) 

wapo_homicide_summary %>% 
  rename("City, State" = city_state, "Unsolved Homicides" = unsolved_homicides, 
         "Total Homicides" = total_homicides) %>% 
  knitr::kable()
```

## Proportion of unsolved homicides in Baltimore

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.


```{r}
## Creating the function to run prop.test and applying broom::tidy
prop_homicide = function(df) {
  prop_test_homicide = prop.test(df$unsolved_homicides, df$total_homicides, 
            alternative = "two.sided", conf.level = 0.95, correct = FALSE)
  
  broom::tidy(prop_test_homicide) %>% 
    select(estimate, conf.low, conf.high)
}

## Applying the prop.test function applied to Baltimore, MD
wapo_homicide_summary %>% 
  filter(city_state == "Baltimore, MD") %>% 
  prop_homicide() %>% 
  rename("Proportion of Unsolved Cases in Baltimore" = estimate, 
         "Lower bound (95% CI)" = conf.low, 
         "Upper bound (95% CI)" = conf.high) %>% 
  knitr::kable()
```

## Proportion of unsolved homicides in all cities
Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and  unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
  wapo_homicide_summary = 
  wapo_homicide_summary %>% 
  nest(unsolved_homicides:total_homicides) %>% 
  mutate(prop_tests = map(.x = .$data, ~prop_homicide(.x))) %>% 
  unnest() %>% 
  janitor::clean_names()

wapo_homicide_summary %>% 
  rename("City, State" = city_state, "Unsolved Homicides" = unsolved_homicides, "Total Homicides" = total_homicides, "Proportion of Unsolved Cases in Baltimore" = estimate, "Lower bound (95% CI)" = conf_low, "Upper bound (95% CI)" = conf_high) %>% 
  knitr::kable()
```

## Create a plot
Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
wapo_homicide_summary %>% 
  mutate(city_state = forcats::fct_reorder(city_state, desc(estimate))) %>%
  ggplot(aes(x = city_state, y = estimate, color = city_state)) + 
    geom_point() +
    geom_errorbar(aes(ymin = conf_low, ymax = conf_high, width = 0.2)) + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
    labs(
        title = "Proportion of Unsolved Homicides in 50 Large Cities",
        x = "City, State",
        y = "Proportion of Unsolved Cases", 
        caption = "Data from the Washington Post"
      ) +
    theme(legend.position = "none") +
    viridis::scale_color_viridis(
      discrete = TRUE
    )
```

The city with the largest proportion of unsolved homicides is Chicago and the lowest proportion of unsolved homicides is Richmond Virginia. 
